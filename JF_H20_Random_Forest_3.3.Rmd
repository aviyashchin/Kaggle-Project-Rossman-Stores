---
title: "Rossmann_H2O_Random_Forest_model_3.2"
author: "David Comfort"
date: "November 18, 2015"
output: html_document
---

Forecast sales using store, promotion, and competitor data

Rossmann Store Sales

Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.

In their first Kaggle competition, Rossmann is challenging you to predict 6 weeks of daily sales for 1,115 stores located across Germany. Reliable sales forecasts enable store managers to create effective staff schedules that increase productivity and motivation. By helping Rossmann create a robust prediction model, you will help store managers stay focused on whatâ€™s most important to them: their customers and their teams!

H2O Random Forest Starter Script
Based on Ben Hamner script from Springleaf
https://www.kaggle.com/benhamner/springleaf-marketing-response/random-forest-example
Forked from Random Forest Example by Michael Pawlus

Use data.table and H2O to create random forest predictions for the entire training set. This is a starter script so it mostly uses the provided fields, as is.

* A log transform has been applied to the Sales, year and month are used, and the Store ID is used as a factor.
* Version 18 does not remove the 0 readings, which does help, since we are not judged on those entries. 
* The model found "Open" to be the most important feature, which makes sense, since Sales are 0 when the store is not open. However, since observations with 0 Sales are discarded by Kaggle upon judging, it is a better strategy to remove them, as Michael's original script does.
* And version 19/20 started removing the 0 records for 0.004 improvement. 
* To make the benchmark a little more competitive, this has more and deeper trees than the original. If you want to see it run faster, you can lower those settings while you work through some other parts of the model, and increase them later.
* Also, the h2o.gbm() has many shared parameters, so you can try that out as well,  and these parameters will work (though you probably don't want depth 30 for GBM).
* And to add variety, you can try out h2o.glm(), a regularized GLM, or h2o.deeplearning(), for deep neural networks. * * This code will work for either with the exception of the ntrees, max_depth, and nbins_cats, which are decision tree  parameters.
* Good luck!

```{r}
library(data.table)  
library(h2o)
library(dplyr)

# localH2O = h2o.init()
localH2O = h2o.init(nthreads = -1, max_mem_size = '4g')

cat("reading the train and test data (with data.table) \n")
train <- fread("../data/train.csv",stringsAsFactors = T)
#str(train)
dd = fread("../data/train.csv",stringsAsFactors = T)
test  <- fread("../data/test.csv",stringsAsFactors = T)
#str(test)

store <- fread("data/store.csv",stringsAsFactors = T)
train <- train[Sales > 0,]  ## We are not judged on 0 sales records in test set
## See Scripts discussion from 10/8 for more explanation.
```

** Merge Train and Test with Store

```{r}
train <- merge(train,store,by="Store")
str(train)
test <- merge(test,store,by="Store")
str(test)
```

Summarize the Data:

```{r}
cat("train data column names and details\n")
summary(train)
cat("test data column names and details\n")
summary(test)
```

* More care should be taken to ensure the dates of test can be projected from train
* Decision trees do not project well, so you will want to have some strategy here, if using the dates

```{r}
train[,Date:=as.Date(Date)]
test[,Date:=as.Date(Date)]
```

* Separating out the elements of the date column for the train set

```{r}
train[,Store:=as.factor(as.numeric(Store))]
test[,Store:=as.factor(as.numeric(Store))]
```

* log transformation to not be as sensitive to high sales
*  decent rule of thumb:if the data spans an order of magnitude, consider a log transform
```{r}
train[,logSales:=log1p(Sales)]
```

## Load data into cluster from R

```{r}
trainHex<-as.h2o(train)
```

## Set up variable to use all features other than those specified here

```{r}
features<-colnames(train)[!(colnames(train) %in% c( "V1","Id","Date","Sales","logSales","Customers"))]
```

## Train a random forest using all default parameters
```{r}
trees = 300
depth = 25
rfHex <- h2o.randomForest(x=features,
                          y="logSales", 
                          ntrees = trees, 
                          max_depth = depth,  
                          nbins_cats = 1115, ## allow it to fit store ID
                          training_frame=trainHex)

```

```{r}
summary(rfHex)

varimps = data.frame(h2o.varimp(rfHex))
varimps
cat("Predicting Sales\n")
```

## Saving our model:
```{r}
rfHex_30_60 = rfHex
h2o.saveModel(rfHex_30_60, path = '/Users/jfdarre/Documents/NYCDS/Project4/H2O_models', force = FALSE)

rfHex_50_65 = rfHex
h2o.saveModel(rfHex_50_65, path = '/Users/jfdarre/Documents/NYCDS/Project4/H2O_models', force = FALSE)

rfHex_50_55_1 = rfHex
h2o.saveModel(rfHex_50_55_1, path = '/Users/jfdarre/Documents/NYCDS/Project4/H2O_models', force = FALSE)

rfHex_50_55_2 = rfHex
h2o.saveModel(rfHex_50_55_2, path = '/Users/jfdarre/Documents/NYCDS/Project4/H2O_models', force = FALSE)

rfHex_50_55_3 = rfHex
h2o.saveModel(rfHex_50_55_3, path = '/Users/jfdarre/Documents/NYCDS/Project4/H2O_models', force = FALSE)

rfHex_30_50_3 = rfHex
h2o.saveModel(rfHex_30_50_3, path = '/Users/jfdarre/Documents/NYCDS/Project4/H2O_models_v2', force = FALSE)

```

## Testing training score:
```{r}
rmse = function(predictions, targets) {
  return(((predictions - targets)/targets) ** 2)
}
```

## Running training errors
```{r}
train_pred = as.data.frame(h2o.predict(rfHex,trainHex))
train_pred <- expm1(train_pred[,1])

train_pred = train_pred
train$pred = train_pred
train$rmse = rmse(train_pred, train$Sales)
train2 = filter(train, month %in% c(8,9))
(total_rmse = sqrt(sum(train$rmse)/nrow(train)))
(partial_rmse = sqrt(sum(train2$rmse)/nrow(train2)))
sumup = as.data.frame(rbind(summary(train_pred), summary(train$Sales), summary(train2$pred), summary(train2$Sales)))
sumup$sd = c(round(sd(train_pred)), round(sd(train$Sales)), round(sd(train2$pred)), round(sd(train2$Sales)))
sumup
```

## Load test data into cluster from R
```{r}
testHex<-as.h2o(test)
```

## Get predictions out; predicts in H2O, as.data.frame gets them into R
```{r}
predictions<-as.data.frame(h2o.predict(rfHex,testHex))
```

## Return the predictions to the original scale of the Sales data
```{r}
pred <- expm1(predictions[,1])
submission <- data.frame(Id=test$Id, Sales=pred)
cat("saving the submission file\n")
write.csv(submission, "./H2O_submits/h2o_30_50_v2.csv",row.names=F)
```

## Doing predictions one at a time and updating the test table with sales:
```{r}
new_pred = data.frame()
temp = filter(test, day_of_year == 213)
test2 = temp
for (i in 214:261) {
  temp = as.h2o(temp)
  temp_pred = as.data.frame(h2o.predict(rfHex,temp))
  new_pred = rbind(new_pred, temp_pred)
  
  if (i == 261) { next }
  temp = filter(test, day_of_year == i)
  temptest = filter(test2, day_of_year == i-1)
  temp$Sales1 = as.data.frame(expm1(temp_pred))
  for (j in 2:21) {
    temp[[paste0('Sales',j)]] = temptest[[paste0('Sales',j-1)]]
  }
  test2 = rbind(test2, temp)
}

new_pred <- expm1(new_pred[,1])
new_submission <- data.frame(Id=test$Id, Sales=new_pred)
cat("saving the submission file\n")
write.csv(new_submission, "./H2O_submits/h2o_30_50_v4.csv",row.names=F)
```



## Scratch code to test things
```{r}
aaa=filter(test, day_of_year == 213)
aaa$Sales1[846:856]
round(pred[846:856])
round(new_pred[846:856])
unique(test$day_of_year)

length(pred)
nrow(new_pred)

class(pred)
class(new_pred)

select(test,contains("Sales"))
select(test2,contains("Sales"))

sum(is.na(test$Sales1))
sum(is.na(test2$Sales1))
sum(is.na(test2$Sales2))

unique(train$Open)
aa = filter(train, Store == 10)
bb = filter(test2, Store == 10)
aa = aa$Sales
bb = bb$Sales1
cc = c(aa,bb)
plot(cc)
```



## Appendix: h2o.randomForest API
* h2o.randomForest(x, y, training_frame, model_id, validation_frame, checkpoint,
* mtries = -1, sample_rate = 0.632, build_tree_one_node = FALSE,
* ntrees = 50, max_depth = 20, min_rows = 1, nbins = 20,
* nbins_cats = 1024, binomial_double_trees = FALSE,
* balance_classes = FALSE, max_after_balance_size = 5, seed,
* offset_column = NULL, weights_column = NULL, nfolds = 0,
* fold_column = NULL, fold_assignment = c("AUTO", "Random", "Modulo"),
* keep_cross_validation_predictions = FALSE, ...)
* Arguments

* x	
* A vector containing the names or indices of the predictor variables to use in building the GBM model.

* y	
* The name or index of the response variable. If the data does not contain a header, this is the column index number starting at 1, and increasing from left to right. (The response must be either an integer or a categorical variable).

* training_frame	
* An H2OFrame object containing the variables in the model.

* model_id	
* (Optional) The unique id assigned to the resulting model. If none is given, an id will automatically be generated.

* validation_frame	
* An H2OFrame object containing the variables in the model.

* checkpoint	
* "Model checkpoint (either key or H2ODeepLearningModel) to resume training with."

* mtries	
* Number of variables randomly sampled as candidates at each split. If set to -1, defaults to sqrtp for classification, and p/3 for regression, where p is the number of predictors.

* sample_rate	
* Sample rate, from 0 to 1.0.   (edit: row sampling, per tree)

* build_tree_one_node	
* Run on one node only; no network overhead but fewer cpus used. Suitable for small datasets.

* ntrees	
* A nonnegative integer that determines the number of trees to grow.

* max_depth	
* Maximum depth to grow the tree.

* min_rows	
* Minimum number of rows to assign to teminal nodes.

* nbins	
* For numerical columns (real/int), build a histogram of this many bins, then split at the best point.

* nbins_cats	
* For categorical columns (enum), build a histogram of this many bins, then split at the best point. Higher values can lead to more overfitting.

* binomial_double_trees	
* For binary classification: Build 2x as many trees (one per class) - can lead to higher accuracy.

* balance_classes	
* logical, indicates whether or not to balance training data class counts via over/under-sampling (for imbalanced data)

* max_after_balance_size	
* Maximum relative size of the training data after balancing class counts (can be less than 1.0)

* seed	
* Seed for random numbers (affects sampling) - Note: only reproducible when running single threaded

* offset_column	
* Specify the offset column.

* weights_column	
* Specify the weights column.

* nfolds	
* (Optional) Number of folds for cross-validation. If nfolds >= 2, then validation must remain empty.

* fold_column	
* (Optional) Column with cross-validation fold index assignment per observation

* fold_assignment	
* Cross-validation fold assignment scheme, if fold_column is not specified Must be "AUTO", "Random" or "Modulo"

* keep_cross_validation_predictions	
* Whether to keep the predictions of the cross-validation models

